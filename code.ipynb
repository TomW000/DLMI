{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":93787,"databundleVersionId":11165379,"sourceType":"competition"}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py\nfrom transformers import CLIPProcessor, CLIPModel\nimport torch\nfrom torch import nn\nimport tqdm \nimport torch.utils.data as utils\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:47:40.154715Z","iopub.execute_input":"2025-04-08T15:47:40.155217Z","iopub.status.idle":"2025-04-08T15:47:50.753372Z","shell.execute_reply.started":"2025-04-08T15:47:40.155168Z","shell.execute_reply":"2025-04-08T15:47:50.752406Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"path = '/kaggle/input/mva-dlmi-2025-histopathology-ood-classification/'\nTRAIN_IMAGES_PATH = path + 'train.h5'\nVAL_IMAGES_PATH = path + 'val.h5'\nTEST_IMAGES_PATH = path + 'test.h5'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:47:50.754832Z","iopub.execute_input":"2025-04-08T15:47:50.755589Z","iopub.status.idle":"2025-04-08T15:47:50.759612Z","shell.execute_reply.started":"2025-04-08T15:47:50.755539Z","shell.execute_reply":"2025-04-08T15:47:50.758697Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available()\n                      else 'cpu'\n) #'mps' if torch.backends.mps.is_available() and torch.backends.mps.is_built() \nscaler = torch.amp.GradScaler('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:47:50.761450Z","iopub.execute_input":"2025-04-08T15:47:50.761769Z","iopub.status.idle":"2025-04-08T15:47:50.832090Z","shell.execute_reply.started":"2025-04-08T15:47:50.761735Z","shell.execute_reply":"2025-04-08T15:47:50.831362Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"'''\nclass CustomDataset(utils.Dataset):\n    def __init__(self, file):\n        self.pixel_values = []\n        self.labels = [] if file != TEST_IMAGES_PATH else None\n        self.processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n\n        with h5py.File(file, 'r') as dataset:\n            for key in dataset.keys():\n                image = dataset[key+'/img'][:]\n                inputs = self.processor(\n                    images=image,\n                    return_tensors=\"pt\",\n                    do_rescale=False,\n                    padding=True\n                )\n                self.pixel_values.append(inputs[\"pixel_values\"].squeeze(0)) \n                \n                if file != TEST_IMAGES_PATH:\n                    label = dataset[key+'/label'][()]\n                    self.labels.append(label)\n        \n    def __len__(self):\n        return len(self.pixel_values) \n    \n    def __getitem__(self, idx):\n        if self.labels is not None: \n            return self.pixel_values[idx], torch.tensor(self.labels[idx], dtype=torch.float32)\n        return self.pixel_values[idx] \n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:47:50.833674Z","iopub.execute_input":"2025-04-08T15:47:50.833933Z","iopub.status.idle":"2025-04-08T15:47:50.850125Z","shell.execute_reply.started":"2025-04-08T15:47:50.833910Z","shell.execute_reply":"2025-04-08T15:47:50.849486Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'\\nclass CustomDataset(utils.Dataset):\\n    def __init__(self, file):\\n        self.pixel_values = []\\n        self.labels = [] if file != TEST_IMAGES_PATH else None\\n        self.processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\\n\\n        with h5py.File(file, \\'r\\') as dataset:\\n            for key in dataset.keys():\\n                image = dataset[key+\\'/img\\'][:]\\n                inputs = self.processor(\\n                    images=image,\\n                    return_tensors=\"pt\",\\n                    do_rescale=False,\\n                    padding=True\\n                )\\n                self.pixel_values.append(inputs[\"pixel_values\"].squeeze(0)) \\n                \\n                if file != TEST_IMAGES_PATH:\\n                    label = dataset[key+\\'/label\\'][()]\\n                    self.labels.append(label)\\n        \\n    def __len__(self):\\n        return len(self.pixel_values) \\n    \\n    def __getitem__(self, idx):\\n        if self.labels is not None: \\n            return self.pixel_values[idx], torch.tensor(self.labels[idx], dtype=torch.float32)\\n        return self.pixel_values[idx] \\n'"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"class CustomDataset(utils.Dataset):\n    def __init__(self, file):\n        self.images = []\n        self.labels = [] if file != TEST_IMAGES_PATH else None\n        with h5py.File(file, 'r') as dataset:\n            for key in dataset.keys():\n                image = dataset[key+'/img'][:]\n                self.images.append(image)\n                if file != TEST_IMAGES_PATH:\n                    label = dataset[key+'/label'][()]            \n                    self.labels.append(label)\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        if self.labels is not None:\n            return torch.from_numpy(self.images[idx]).to(torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n        return torch.from_numpy(self.images[idx]).to(torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:47:50.850840Z","iopub.execute_input":"2025-04-08T15:47:50.851056Z","iopub.status.idle":"2025-04-08T15:47:50.868510Z","shell.execute_reply.started":"2025-04-08T15:47:50.851025Z","shell.execute_reply":"2025-04-08T15:47:50.867825Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def init_model(module):\n    if isinstance(module, nn.Linear):\n        nn.init.xavier_normal_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:47:50.869299Z","iopub.execute_input":"2025-04-08T15:47:50.869518Z","iopub.status.idle":"2025-04-08T15:47:50.885168Z","shell.execute_reply.started":"2025-04-08T15:47:50.869498Z","shell.execute_reply":"2025-04-08T15:47:50.884352Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"'''\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.device = device \n        \n        self.vision_model = CLIPModel.from_pretrained(\"vinid/plip\").vision_model\n        \n        for param in self.vision_model.parameters():\n            param.requires_grad = False\n        self.vision_model.to(self.device)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(768, 384), \n            nn.ReLU(),\n            nn.Linear(384, 192), \n            nn.ReLU(),\n            nn.Linear(192, 48), \n            nn.ReLU(),\n            nn.Linear(48, 1),\n            nn.Sigmoid()\n        )\n        \n        self.apply(init_model)\n        self.to(self.device)\n            \n    def forward(self, pixel_values):\n                \n        with torch.no_grad():\n            embedding = self.vision_model(pixel_values=pixel_values).last_hidden_state[:, 0, :]\n        \n        return self.classifier(embedding)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:47:50.885868Z","iopub.execute_input":"2025-04-08T15:47:50.886072Z","iopub.status.idle":"2025-04-08T15:47:50.903134Z","shell.execute_reply.started":"2025-04-08T15:47:50.886042Z","shell.execute_reply":"2025-04-08T15:47:50.902495Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'\\nclass Model(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.device = device \\n        \\n        self.vision_model = CLIPModel.from_pretrained(\"vinid/plip\").vision_model\\n        \\n        for param in self.vision_model.parameters():\\n            param.requires_grad = False\\n        self.vision_model.to(self.device)\\n\\n        self.classifier = nn.Sequential(\\n            nn.Linear(768, 384), \\n            nn.ReLU(),\\n            nn.Linear(384, 192), \\n            nn.ReLU(),\\n            nn.Linear(192, 48), \\n            nn.ReLU(),\\n            nn.Linear(48, 1),\\n            nn.Sigmoid()\\n        )\\n        \\n        self.apply(init_model)\\n        self.to(self.device)\\n            \\n    def forward(self, pixel_values):\\n                \\n        with torch.no_grad():\\n            embedding = self.vision_model(pixel_values=pixel_values).last_hidden_state[:, 0, :]\\n        \\n        return self.classifier(embedding)\\n'"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.device = device \n        \n        self.processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n        self.vision_model = CLIPModel.from_pretrained(\"vinid/plip\").vision_model\n        \n        for param in self.vision_model.parameters():\n            param.requires_grad = False\n        self.vision_model.to(self.device)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(768, 384), \n            nn.ReLU(),\n            nn.Linear(384, 192), \n            nn.ReLU(),\n            nn.Linear(192, 48), \n            nn.ReLU(),\n            nn.Linear(48, 1),\n        )\n        \n        self.apply(init_model)\n        self.to(self.device)\n            \n    def forward(self, image):\n        inputs = self.processor(\n            images=image, \n            return_tensors=\"pt\", \n            padding=True, \n            do_rescale=False\n        )\n        \n        pixel_values = inputs['pixel_values'].to(self.device, dtype=torch.float32)\n        \n        with torch.no_grad():\n            embedding = self.vision_model(pixel_values=pixel_values).last_hidden_state[:, 0, :]\n        \n        return self.classifier(embedding)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:56:19.326747Z","iopub.execute_input":"2025-04-08T15:56:19.327081Z","iopub.status.idle":"2025-04-08T15:56:19.333950Z","shell.execute_reply.started":"2025-04-08T15:56:19.327055Z","shell.execute_reply":"2025-04-08T15:56:19.332897Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_batch_size, val_batch_size, test_batch_size = 1000, 680, 850\n\ntrain_dataset = CustomDataset(TRAIN_IMAGES_PATH) \nval_dataset = CustomDataset(VAL_IMAGES_PATH)\ntest_dataset = CustomDataset(TEST_IMAGES_PATH)\n\ntrain_loader = utils.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=8, pin_memory=True)\nval_loader = utils.DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True, num_workers=6, pin_memory=True)\ntest_loader = utils.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:47:50.920478Z","iopub.execute_input":"2025-04-08T15:47:50.920773Z","iopub.status.idle":"2025-04-08T15:52:40.849712Z","shell.execute_reply.started":"2025-04-08T15:47:50.920742Z","shell.execute_reply":"2025-04-08T15:52:40.848744Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"model = Model()\nmodel.to(device)\nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.AdamW(model.classifier.parameters(), lr=1e-4, weight_decay=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:56:50.514998Z","iopub.execute_input":"2025-04-08T15:56:50.515395Z","iopub.status.idle":"2025-04-08T15:56:52.333878Z","shell.execute_reply.started":"2025-04-08T15:56:50.515358Z","shell.execute_reply":"2025-04-08T15:56:52.332577Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def calculate_accuracy(predictions, labels):\n    binary_preds = (predictions >= 0.5).float()\n    correct = (binary_preds == labels).float()\n    accuracy = correct.mean() * 100\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:52:46.512300Z","iopub.execute_input":"2025-04-08T15:52:46.512727Z","iopub.status.idle":"2025-04-08T15:52:46.518009Z","shell.execute_reply.started":"2025-04-08T15:52:46.512662Z","shell.execute_reply":"2025-04-08T15:52:46.516951Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def use(epochs):\n    train_accs, val_accs = [], []\n    \n    for epoch in tqdm.tqdm(range(epochs), desc='Epochs'):\n        model.train()\n        total_acc = 0.0\n        \n        for images, labels in tqdm.tqdm(train_loader, desc=f'Train {epoch+1}'):\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n\n            \n            with torch.amp.autocast('cuda'):\n\n                y_pred = model(images).squeeze()\n                loss = loss_fn(y_pred, labels)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            acc = calculate_accuracy(y_pred, labels)\n            total_acc += acc\n\n        train_acc = total_acc / (len(train_loader))\n        train_accs.append(train_acc)\n\n\n        model.eval()\n        total_val_acc = 0.0\n        \n        with torch.no_grad():\n            for images, labels in tqdm.tqdm(val_loader, desc=f'Val {epoch+1}'):\n                images, labels = images.to(device), labels.to(device)\n                \n                with torch.amp.autocast('cuda'):\n                    val_pred = model(images).squeeze()\n                \n                val_acc = calculate_accuracy(val_pred, labels)\n                total_val_acc += val_acc\n\n        val_acc = total_val_acc / len(val_loader)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch{epoch+1}/{epochs} - Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n\n\n    test_predictions = []\n    \n    with h5py.File(TEST_IMAGES_PATH, 'r') as f:\n        idx = list(f.keys())\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for images in tqdm.tqdm(test_loader, desc='Testing'):\n            images = images.to(device)\n            \n            with torch.amp.autocast('cuda'):\n                test_pred = (model(images).squeeze() > 0.5).float()\n            test_predictions.append(test_pred)\n            \n        test = list(torch.cat(test_predictions).cpu().numpy())\n\n    return list(zip(idx, test)) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:52:46.518971Z","iopub.execute_input":"2025-04-08T15:52:46.519219Z","iopub.status.idle":"2025-04-08T15:52:46.540174Z","shell.execute_reply.started":"2025-04-08T15:52:46.519198Z","shell.execute_reply":"2025-04-08T15:52:46.539241Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"result = use(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:56:59.800811Z","iopub.execute_input":"2025-04-08T15:56:59.801140Z","iopub.status.idle":"2025-04-08T16:15:54.567672Z","shell.execute_reply.started":"2025-04-08T15:56:59.801112Z","shell.execute_reply":"2025-04-08T16:15:54.566499Z"}},"outputs":[{"name":"stderr","text":"Epochs:   0%|          | 0/1 [00:00<?, ?it/s]\nTrain 1:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\nTrain 1:   1%|          | 1/100 [00:09<15:58,  9.68s/it]\u001b[A\nTrain 1:   2%|▏         | 2/100 [00:14<11:21,  6.96s/it]\u001b[A\nTrain 1:   3%|▎         | 3/100 [00:19<09:48,  6.07s/it]\u001b[A\nTrain 1:   4%|▍         | 4/100 [00:24<08:57,  5.60s/it]\u001b[A\nTrain 1:   5%|▌         | 5/100 [00:29<08:30,  5.37s/it]\u001b[A\nTrain 1:   6%|▌         | 6/100 [00:34<08:10,  5.21s/it]\u001b[A\nTrain 1:   7%|▋         | 7/100 [00:39<07:57,  5.13s/it]\u001b[A\nTrain 1:   8%|▊         | 8/100 [00:44<07:46,  5.07s/it]\u001b[A\nTrain 1:   9%|▉         | 9/100 [00:49<07:39,  5.05s/it]\u001b[A\nTrain 1:  10%|█         | 10/100 [00:54<07:29,  5.00s/it]\u001b[A\nTrain 1:  11%|█         | 11/100 [00:59<07:25,  5.01s/it]\u001b[A\nTrain 1:  12%|█▏        | 12/100 [01:04<07:19,  4.99s/it]\u001b[A\nTrain 1:  13%|█▎        | 13/100 [01:09<07:14,  5.00s/it]\u001b[A\nTrain 1:  14%|█▍        | 14/100 [01:14<07:26,  5.19s/it]\u001b[A\nTrain 1:  15%|█▌        | 15/100 [01:19<07:16,  5.14s/it]\u001b[A\nTrain 1:  16%|█▌        | 16/100 [01:24<07:04,  5.05s/it]\u001b[A\nTrain 1:  17%|█▋        | 17/100 [01:29<06:56,  5.02s/it]\u001b[A\nTrain 1:  18%|█▊        | 18/100 [01:34<06:47,  4.97s/it]\u001b[A\nTrain 1:  19%|█▉        | 19/100 [01:39<06:42,  4.97s/it]\u001b[A\nTrain 1:  20%|██        | 20/100 [01:44<06:43,  5.04s/it]\u001b[A\nTrain 1:  21%|██        | 21/100 [01:49<06:36,  5.02s/it]\u001b[A\nTrain 1:  22%|██▏       | 22/100 [01:54<06:28,  4.98s/it]\u001b[A\nTrain 1:  23%|██▎       | 23/100 [01:59<06:23,  4.97s/it]\u001b[A\nTrain 1:  24%|██▍       | 24/100 [02:04<06:17,  4.97s/it]\u001b[A\nTrain 1:  25%|██▌       | 25/100 [02:09<06:12,  4.96s/it]\u001b[A\nTrain 1:  26%|██▌       | 26/100 [02:14<06:05,  4.94s/it]\u001b[A\nTrain 1:  27%|██▋       | 27/100 [02:19<06:06,  5.02s/it]\u001b[A\nTrain 1:  28%|██▊       | 28/100 [02:24<05:58,  4.98s/it]\u001b[A\nTrain 1:  29%|██▉       | 29/100 [02:29<05:52,  4.97s/it]\u001b[A\nTrain 1:  30%|███       | 30/100 [02:34<05:49,  4.99s/it]\u001b[A\nTrain 1:  31%|███       | 31/100 [02:39<05:43,  4.97s/it]\u001b[A\nTrain 1:  32%|███▏      | 32/100 [02:44<05:36,  4.95s/it]\u001b[A\nTrain 1:  33%|███▎      | 33/100 [02:49<05:46,  5.18s/it]\u001b[A\nTrain 1:  34%|███▍      | 34/100 [02:54<05:36,  5.10s/it]\u001b[A\nTrain 1:  35%|███▌      | 35/100 [02:59<05:28,  5.06s/it]\u001b[A\nTrain 1:  36%|███▌      | 36/100 [03:04<05:20,  5.02s/it]\u001b[A\nTrain 1:  37%|███▋      | 37/100 [03:09<05:15,  5.01s/it]\u001b[A\nTrain 1:  38%|███▊      | 38/100 [03:14<05:08,  4.98s/it]\u001b[A\nTrain 1:  39%|███▉      | 39/100 [03:19<05:03,  4.98s/it]\u001b[A\nTrain 1:  40%|████      | 40/100 [03:24<04:57,  4.96s/it]\u001b[A\nTrain 1:  41%|████      | 41/100 [03:29<04:53,  4.98s/it]\u001b[A\nTrain 1:  42%|████▏     | 42/100 [03:34<04:46,  4.95s/it]\u001b[A\nTrain 1:  43%|████▎     | 43/100 [03:39<04:41,  4.94s/it]\u001b[A\nTrain 1:  44%|████▍     | 44/100 [03:44<04:36,  4.94s/it]\u001b[A\nTrain 1:  45%|████▌     | 45/100 [03:49<04:32,  4.95s/it]\u001b[A\nTrain 1:  46%|████▌     | 46/100 [03:54<04:28,  4.98s/it]\u001b[A\nTrain 1:  47%|████▋     | 47/100 [03:59<04:23,  4.97s/it]\u001b[A\nTrain 1:  48%|████▊     | 48/100 [04:04<04:17,  4.96s/it]\u001b[A\nTrain 1:  49%|████▉     | 49/100 [04:09<04:13,  4.96s/it]\u001b[A\nTrain 1:  50%|█████     | 50/100 [04:14<04:07,  4.95s/it]\u001b[A\nTrain 1:  51%|█████     | 51/100 [04:19<04:02,  4.94s/it]\u001b[A\nTrain 1:  52%|█████▏    | 52/100 [04:24<04:01,  5.03s/it]\u001b[A\nTrain 1:  53%|█████▎    | 53/100 [04:29<03:58,  5.07s/it]\u001b[A\nTrain 1:  54%|█████▍    | 54/100 [04:34<03:50,  5.02s/it]\u001b[A\nTrain 1:  55%|█████▌    | 55/100 [04:39<03:45,  5.00s/it]\u001b[A\nTrain 1:  56%|█████▌    | 56/100 [04:44<03:38,  4.97s/it]\u001b[A\nTrain 1:  57%|█████▋    | 57/100 [04:49<03:33,  4.97s/it]\u001b[A\nTrain 1:  58%|█████▊    | 58/100 [04:54<03:27,  4.94s/it]\u001b[A\nTrain 1:  59%|█████▉    | 59/100 [04:59<03:26,  5.03s/it]\u001b[A\nTrain 1:  60%|██████    | 60/100 [05:04<03:19,  4.99s/it]\u001b[A\nTrain 1:  61%|██████    | 61/100 [05:09<03:14,  4.99s/it]\u001b[A\nTrain 1:  62%|██████▏   | 62/100 [05:14<03:08,  4.95s/it]\u001b[A\nTrain 1:  63%|██████▎   | 63/100 [05:18<03:03,  4.95s/it]\u001b[A\nTrain 1:  64%|██████▍   | 64/100 [05:23<02:57,  4.94s/it]\u001b[A\nTrain 1:  65%|██████▌   | 65/100 [05:29<02:58,  5.10s/it]\u001b[A\nTrain 1:  66%|██████▌   | 66/100 [05:34<02:52,  5.07s/it]\u001b[A\nTrain 1:  67%|██████▋   | 67/100 [05:39<02:45,  5.03s/it]\u001b[A\nTrain 1:  68%|██████▊   | 68/100 [05:44<02:39,  4.98s/it]\u001b[A\nTrain 1:  69%|██████▉   | 69/100 [05:49<02:34,  4.97s/it]\u001b[A\nTrain 1:  70%|███████   | 70/100 [05:54<02:28,  4.94s/it]\u001b[A\nTrain 1:  71%|███████   | 71/100 [05:58<02:23,  4.95s/it]\u001b[A\nTrain 1:  72%|███████▏  | 72/100 [06:03<02:18,  4.94s/it]\u001b[A\nTrain 1:  73%|███████▎  | 73/100 [06:08<02:13,  4.95s/it]\u001b[A\nTrain 1:  74%|███████▍  | 74/100 [06:13<02:08,  4.94s/it]\u001b[A\nTrain 1:  75%|███████▌  | 75/100 [06:18<02:03,  4.95s/it]\u001b[A\nTrain 1:  76%|███████▌  | 76/100 [06:23<01:58,  4.94s/it]\u001b[A\nTrain 1:  77%|███████▋  | 77/100 [06:28<01:54,  4.96s/it]\u001b[A\nTrain 1:  78%|███████▊  | 78/100 [06:33<01:50,  5.02s/it]\u001b[A\nTrain 1:  79%|███████▉  | 79/100 [06:38<01:45,  5.02s/it]\u001b[A\nTrain 1:  80%|████████  | 80/100 [06:43<01:39,  4.99s/it]\u001b[A\nTrain 1:  81%|████████  | 81/100 [06:48<01:34,  4.99s/it]\u001b[A\nTrain 1:  82%|████████▏ | 82/100 [06:53<01:29,  4.96s/it]\u001b[A\nTrain 1:  83%|████████▎ | 83/100 [06:58<01:24,  4.97s/it]\u001b[A\nTrain 1:  84%|████████▍ | 84/100 [07:03<01:20,  5.02s/it]\u001b[A\nTrain 1:  85%|████████▌ | 85/100 [07:08<01:15,  5.01s/it]\u001b[A\nTrain 1:  86%|████████▌ | 86/100 [07:13<01:09,  4.97s/it]\u001b[A\nTrain 1:  87%|████████▋ | 87/100 [07:18<01:04,  4.96s/it]\u001b[A\nTrain 1:  88%|████████▊ | 88/100 [07:23<00:59,  4.93s/it]\u001b[A\nTrain 1:  89%|████████▉ | 89/100 [07:28<00:54,  4.92s/it]\u001b[A\nTrain 1:  90%|█████████ | 90/100 [07:33<00:49,  4.91s/it]\u001b[A\nTrain 1:  91%|█████████ | 91/100 [07:38<00:44,  4.97s/it]\u001b[A\nTrain 1:  92%|█████████▏| 92/100 [07:43<00:39,  4.94s/it]\u001b[A\nTrain 1:  93%|█████████▎| 93/100 [07:48<00:34,  4.94s/it]\u001b[A\nTrain 1:  94%|█████████▍| 94/100 [07:52<00:29,  4.90s/it]\u001b[A\nTrain 1:  95%|█████████▌| 95/100 [07:57<00:24,  4.92s/it]\u001b[A\nTrain 1:  96%|█████████▌| 96/100 [08:02<00:19,  4.91s/it]\u001b[A\nTrain 1:  97%|█████████▋| 97/100 [08:08<00:15,  5.04s/it]\u001b[A\nTrain 1:  98%|█████████▊| 98/100 [08:13<00:09,  4.99s/it]\u001b[A\nTrain 1:  99%|█████████▉| 99/100 [08:17<00:04,  4.98s/it]\u001b[A\nTrain 1: 100%|██████████| 100/100 [08:23<00:00,  5.04s/it]\u001b[A\n\nVal 1:   0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\nVal 1:   2%|▏         | 1/52 [00:04<04:05,  4.82s/it]\u001b[A\nVal 1:   4%|▍         | 2/52 [00:08<03:15,  3.90s/it]\u001b[A\nVal 1:   6%|▌         | 3/52 [00:11<02:56,  3.61s/it]\u001b[A\nVal 1:   8%|▊         | 4/52 [00:14<02:53,  3.61s/it]\u001b[A\nVal 1:  10%|▉         | 5/52 [00:18<02:43,  3.48s/it]\u001b[A\nVal 1:  12%|█▏        | 6/52 [00:21<02:36,  3.40s/it]\u001b[A\nVal 1:  13%|█▎        | 7/52 [00:24<02:30,  3.35s/it]\u001b[A\nVal 1:  15%|█▌        | 8/52 [00:27<02:25,  3.32s/it]\u001b[A\nVal 1:  17%|█▋        | 9/52 [00:31<02:21,  3.29s/it]\u001b[A\nVal 1:  19%|█▉        | 10/52 [00:34<02:18,  3.30s/it]\u001b[A\nVal 1:  21%|██        | 11/52 [00:37<02:14,  3.27s/it]\u001b[A\nVal 1:  23%|██▎       | 12/52 [00:40<02:11,  3.28s/it]\u001b[A\nVal 1:  25%|██▌       | 13/52 [00:44<02:08,  3.29s/it]\u001b[A\nVal 1:  27%|██▋       | 14/52 [00:47<02:05,  3.29s/it]\u001b[A\nVal 1:  29%|██▉       | 15/52 [00:50<02:01,  3.28s/it]\u001b[A\nVal 1:  31%|███       | 16/52 [00:54<01:58,  3.29s/it]\u001b[A\nVal 1:  33%|███▎      | 17/52 [00:57<01:54,  3.28s/it]\u001b[A\nVal 1:  35%|███▍      | 18/52 [01:00<01:51,  3.27s/it]\u001b[A\nVal 1:  37%|███▋      | 19/52 [01:03<01:48,  3.27s/it]\u001b[A\nVal 1:  38%|███▊      | 20/52 [01:07<01:44,  3.26s/it]\u001b[A\nVal 1:  40%|████      | 21/52 [01:10<01:41,  3.26s/it]\u001b[A\nVal 1:  42%|████▏     | 22/52 [01:13<01:38,  3.27s/it]\u001b[A\nVal 1:  44%|████▍     | 23/52 [01:17<01:35,  3.30s/it]\u001b[A\nVal 1:  46%|████▌     | 24/52 [01:20<01:31,  3.28s/it]\u001b[A\nVal 1:  48%|████▊     | 25/52 [01:23<01:28,  3.27s/it]\u001b[A\nVal 1:  50%|█████     | 26/52 [01:26<01:24,  3.25s/it]\u001b[A\nVal 1:  52%|█████▏    | 27/52 [01:30<01:21,  3.25s/it]\u001b[A\nVal 1:  54%|█████▍    | 28/52 [01:33<01:18,  3.26s/it]\u001b[A\nVal 1:  56%|█████▌    | 29/52 [01:36<01:14,  3.26s/it]\u001b[A\nVal 1:  58%|█████▊    | 30/52 [01:39<01:11,  3.26s/it]\u001b[A\nVal 1:  60%|█████▉    | 31/52 [01:43<01:08,  3.28s/it]\u001b[A\nVal 1:  62%|██████▏   | 32/52 [01:46<01:05,  3.27s/it]\u001b[A\nVal 1:  63%|██████▎   | 33/52 [01:49<01:03,  3.34s/it]\u001b[A\nVal 1:  65%|██████▌   | 34/52 [01:53<00:59,  3.33s/it]\u001b[A\nVal 1:  67%|██████▋   | 35/52 [01:56<00:56,  3.30s/it]\u001b[A\nVal 1:  69%|██████▉   | 36/52 [01:59<00:52,  3.28s/it]\u001b[A\nVal 1:  71%|███████   | 37/52 [02:03<00:49,  3.29s/it]\u001b[A\nVal 1:  73%|███████▎  | 38/52 [02:06<00:45,  3.27s/it]\u001b[A\nVal 1:  75%|███████▌  | 39/52 [02:09<00:42,  3.27s/it]\u001b[A\nVal 1:  77%|███████▋  | 40/52 [02:12<00:39,  3.26s/it]\u001b[A\nVal 1:  79%|███████▉  | 41/52 [02:16<00:35,  3.26s/it]\u001b[A\nVal 1:  81%|████████  | 42/52 [02:19<00:32,  3.26s/it]\u001b[A\nVal 1:  83%|████████▎ | 43/52 [02:23<00:30,  3.43s/it]\u001b[A\nVal 1:  85%|████████▍ | 44/52 [02:26<00:27,  3.38s/it]\u001b[A\nVal 1:  87%|████████▋ | 45/52 [02:29<00:23,  3.33s/it]\u001b[A\nVal 1:  88%|████████▊ | 46/52 [02:32<00:19,  3.32s/it]\u001b[A\nVal 1:  90%|█████████ | 47/52 [02:36<00:16,  3.30s/it]\u001b[A\nVal 1:  92%|█████████▏| 48/52 [02:39<00:13,  3.30s/it]\u001b[A\nVal 1:  94%|█████████▍| 49/52 [02:42<00:09,  3.31s/it]\u001b[A\nVal 1:  96%|█████████▌| 50/52 [02:46<00:06,  3.29s/it]\u001b[A\nVal 1:  98%|█████████▊| 51/52 [02:49<00:03,  3.28s/it]\u001b[A\nVal 1: 100%|██████████| 52/52 [02:51<00:00,  3.30s/it]\u001b[A\nEpochs: 100%|██████████| 1/1 [11:14<00:00, 674.99s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch1/1 - Train Acc: 83.9210, Val Acc: 81.1412\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 101/101 [07:01<00:00,  4.18s/it]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"df = pd.DataFrame(result, columns=['ID', 'Pred'])\ndf.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T16:23:44.650396Z","iopub.execute_input":"2025-04-08T16:23:44.650822Z","iopub.status.idle":"2025-04-08T16:23:44.761548Z","shell.execute_reply.started":"2025-04-08T16:23:44.650787Z","shell.execute_reply":"2025-04-08T16:23:44.760533Z"}},"outputs":[],"execution_count":25}]}