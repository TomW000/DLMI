{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":93787,"databundleVersionId":11165379,"sourceType":"competition"}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py\nfrom transformers import CLIPProcessor, CLIPModel\nimport torch\nfrom torch import nn\nimport tqdm \nimport torch.utils.data as utils\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:26:02.328158Z","iopub.execute_input":"2025-04-08T15:26:02.328643Z","iopub.status.idle":"2025-04-08T15:26:14.613963Z","shell.execute_reply.started":"2025-04-08T15:26:02.328598Z","shell.execute_reply":"2025-04-08T15:26:14.613267Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"path = '/kaggle/input/mva-dlmi-2025-histopathology-ood-classification/'\nTRAIN_IMAGES_PATH = path + 'train.h5'\nVAL_IMAGES_PATH = path + 'val.h5'\nTEST_IMAGES_PATH = path + 'test.h5'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:26:14.615082Z","iopub.execute_input":"2025-04-08T15:26:14.615712Z","iopub.status.idle":"2025-04-08T15:26:14.619450Z","shell.execute_reply.started":"2025-04-08T15:26:14.615672Z","shell.execute_reply":"2025-04-08T15:26:14.618470Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available()\n                      else 'cpu'\n) #'mps' if torch.backends.mps.is_available() and torch.backends.mps.is_built() \nscaler = torch.amp.GradScaler('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:26:14.621106Z","iopub.execute_input":"2025-04-08T15:26:14.621344Z","iopub.status.idle":"2025-04-08T15:26:14.694632Z","shell.execute_reply.started":"2025-04-08T15:26:14.621321Z","shell.execute_reply":"2025-04-08T15:26:14.693731Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"'''\nclass CustomDataset(utils.Dataset):\n    def __init__(self, file):\n        self.pixel_values = []\n        self.labels = [] if file != TEST_IMAGES_PATH else None\n        self.processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n\n        with h5py.File(file, 'r') as dataset:\n            for key in dataset.keys():\n                image = dataset[key+'/img'][:]\n                inputs = self.processor(\n                    images=image,\n                    return_tensors=\"pt\",\n                    do_rescale=False,\n                    padding=True\n                )\n                self.pixel_values.append(inputs[\"pixel_values\"].squeeze(0)) \n                \n                if file != TEST_IMAGES_PATH:\n                    label = dataset[key+'/label'][()]\n                    self.labels.append(label)\n        \n    def __len__(self):\n        return len(self.pixel_values) \n    \n    def __getitem__(self, idx):\n        if self.labels is not None: \n            return self.pixel_values[idx], torch.tensor(self.labels[idx], dtype=torch.float32)\n        return self.pixel_values[idx] \n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:26:14.696109Z","iopub.execute_input":"2025-04-08T15:26:14.696476Z","iopub.status.idle":"2025-04-08T15:26:14.712502Z","shell.execute_reply.started":"2025-04-08T15:26:14.696432Z","shell.execute_reply":"2025-04-08T15:26:14.711634Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class CustomDataset(utils.Dataset):\n    def __init__(self, file):\n        self.images = []\n        self.labels = [] if file != TEST_IMAGES_PATH else None\n        with h5py.File(file, 'r') as dataset:\n            for key in dataset.keys():\n                image = dataset[key+'/img'][:]\n                self.images.append(image)\n                if file != TEST_IMAGES_PATH:\n                    label = dataset[key+'/label'][()]            \n                    self.labels.append(label)\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        if self.labels is not None:\n            return torch.from_numpy(self.images[idx]).to(torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n        return torch.from_numpy(self.images[idx]).to(torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def init_model(module):\n    if isinstance(module, nn.Linear):\n        nn.init.xavier_normal_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:26:14.713397Z","iopub.execute_input":"2025-04-08T15:26:14.713687Z","iopub.status.idle":"2025-04-08T15:26:14.730030Z","shell.execute_reply.started":"2025-04-08T15:26:14.713657Z","shell.execute_reply":"2025-04-08T15:26:14.729361Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"'''\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.device = device \n        \n        self.vision_model = CLIPModel.from_pretrained(\"vinid/plip\").vision_model\n        \n        for param in self.vision_model.parameters():\n            param.requires_grad = False\n        self.vision_model.to(self.device)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(768, 384), \n            nn.ReLU(),\n            nn.Linear(384, 192), \n            nn.ReLU(),\n            nn.Linear(192, 48), \n            nn.ReLU(),\n            nn.Linear(48, 1),\n            nn.Sigmoid()\n        )\n        \n        self.apply(init_model)\n        self.to(self.device)\n            \n    def forward(self, pixel_values):\n                \n        with torch.no_grad():\n            embedding = self.vision_model(pixel_values=pixel_values).last_hidden_state[:, 0, :]\n        \n        return self.classifier(embedding)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:26:14.730754Z","iopub.execute_input":"2025-04-08T15:26:14.730968Z","iopub.status.idle":"2025-04-08T15:26:14.745907Z","shell.execute_reply.started":"2025-04-08T15:26:14.730950Z","shell.execute_reply":"2025-04-08T15:26:14.745127Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.device = device \n        \n        self.processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n        self.vision_model = CLIPModel.from_pretrained(\"vinid/plip\").vision_model\n        \n        for param in self.vision_model.parameters():\n            param.requires_grad = False\n        self.vision_model.to(self.device)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(768, 384), \n            nn.ReLU(),\n            nn.Linear(384, 192), \n            nn.ReLU(),\n            nn.Linear(192, 48), \n            nn.ReLU(),\n            nn.Linear(48, 1),\n            nn.Sigmoid()\n        )\n        \n        self.apply(init_model)\n        self.to(self.device)\n            \n    def forward(self, image):\n        inputs = self.processor(\n            images=image, \n            return_tensors=\"pt\", \n            padding=True, \n            do_rescale=False\n        )\n        \n        pixel_values = inputs['pixel_values'].to(self.device, dtype=torch.float32)\n        \n        with torch.no_grad():\n            embedding = self.vision_model(pixel_values=pixel_values).last_hidden_state[:, 0, :]\n        \n        return self.classifier(embedding)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = CustomDataset(TRAIN_IMAGES_PATH) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T15:26:14.746751Z","iopub.execute_input":"2025-04-08T15:26:14.747030Z","execution_failed":"2025-04-08T15:35:47.060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_dataset = CustomDataset(VAL_IMAGES_PATH)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-08T15:35:47.060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = CustomDataset(TEST_IMAGES_PATH)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-08T15:35:47.060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_batch_size, val_batch_size, test_batch_size = 1000, 680, 850\n\ntrain_loader = utils.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=8, pin_memory=True)\nval_loader = utils.DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True, num_workers=6, pin_memory=True)\ntest_loader = utils.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-08T15:35:47.060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Model()\nmodel.to(device)\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.AdamW(model.classifier.parameters(), lr=1e-4, weight_decay=1e-4)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-08T15:35:47.060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_accuracy(predictions, labels):\n    binary_preds = (predictions >= 0.5).float()\n    correct = (binary_preds == labels).float()\n    accuracy = correct.mean() * 100\n    return accuracy","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-08T15:35:47.060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def use(epochs):\n    train_accs, val_accs = [], []\n    \n    for epoch in tqdm.tqdm(range(epochs), desc='Epochs'):\n        model.train()\n        total_acc = 0.0\n        \n        for pixel_values, labels in tqdm.tqdm(train_loader, desc=f'Train {epoch+1}', leave=True):\n            pixel_values, labels = pixel_values.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n\n            \n            with torch.amp.autocast('cuda'):\n\n                y_pred = model(pixel_values).squeeze()\n                loss = loss_fn(y_pred, labels)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            acc = calculate_accuracy(y_pred, labels)\n            total_acc += acc\n\n        train_acc = total_acc / (len(train_loader))\n        train_accs.append(train_acc)\n\n\n        model.eval()\n        total_val_acc = 0.0\n        \n        with torch.no_grad():\n            for pixel_values, labels in tqdm.tqdm(val_loader, desc=f'Val {epoch+1}', leave=True):\n                pixel_values, labels = pixel_values.to(device), labels.to(device)\n                \n                with torch.amp.autocast('cuda'):\n                    val_pred = model(pixel_values).squeeze()\n                \n                val_acc = calculate_accuracy(val_pred, labels)\n                total_val_acc += val_acc\n\n        val_acc = total_val_acc / len(val_loader)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch{epoch+1}/{epochs} - Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n\n\n    test_predictions = []\n    \n    with h5py.File(TEST_IMAGES_PATH, 'r') as f:\n        idx = list(f.keys())\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for pixel_values in tqdm.tqdm(test_loader, desc='Testing'):\n            pixel_values = pixel_values.to(device)\n            \n            with torch.amp.autocast('cuda'):\n                test_pred = (model(pixel_values).squeeze() > 0.5).float()\n            test_predictions.append(test_pred)\n            \n        test = list(torch.cat(test_predictions).cpu().numpy())\n\n    return list(zip(idx, test)) ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-08T15:35:47.060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = use(1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-08T15:35:47.060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame(result, columns=['ID', 'Pred'])\ndf.to_csv('submission.csv', index = False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-08T15:35:47.061Z"}},"outputs":[],"execution_count":null}]}