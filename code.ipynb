{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import tqdm \n",
    "import torch.utils.data as utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/kaggle/input/mva-dlmi-2025-histopathology-ood-classification/'\n",
    "TRAIN_IMAGES_PATH = path + 'train.h5'\n",
    "VAL_IMAGES_PATH = path + 'val.h5'\n",
    "TEST_IMAGES_PATH = path + 'test.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    'mps' if torch.backends.mps.is_available() and torch.backends.mps.is_built() \n",
    "    else 'cuda' if torch.cuda.is_available() \n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "class CustomDataset(utils.Dataset):\n",
    "    def __init__(self, file):\n",
    "        self.images, self.labels = [], []\n",
    "        self.file = file\n",
    "        with h5py.File(file, 'r') as dataset:\n",
    "            for key in dataset.keys():\n",
    "                image = dataset[key+'/img'][:]\n",
    "                self.images.append(image)\n",
    "                if file != TEST_IMAGES_PATH:\n",
    "                    label = dataset[key+'/label'][()]            \n",
    "                    self.labels.append(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.from_numpy(self.images[idx]).to(torch.float32)\n",
    "        if self.file != TEST_IMAGES_PATH:\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "            return image, label\n",
    "        return image\n",
    "\n",
    "\n",
    "def init_model(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = device \n",
    "        \n",
    "        self.processor = CLIPProcessor.from_pretrained(\"vinid/plip\")\n",
    "        self.vision_model = CLIPModel.from_pretrained(\"vinid/plip\").vision_model\n",
    "        \n",
    "        for param in self.vision_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vision_model.to(self.device)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 384), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(384, 192), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(192, 48), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(48, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.apply(init_model)\n",
    "        self.to(self.device)\n",
    "            \n",
    "    def forward(self, image):\n",
    "        inputs = self.processor(\n",
    "            images=image, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            do_rescale=False\n",
    "        )\n",
    "        \n",
    "        pixel_values = inputs['pixel_values'].to(self.device, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = self.vision_model(pixel_values=pixel_values).last_hidden_state[:, 0, :]\n",
    "        \n",
    "        return self.classifier(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size, val_batch_size, test_batch_size = 100, 10, 10\n",
    "\n",
    "train_dataset = CustomDataset(TRAIN_IMAGES_PATH)\n",
    "val_dataset = CustomDataset(VAL_IMAGES_PATH)\n",
    "test_dataset = CustomDataset(TEST_IMAGES_PATH)\n",
    "\n",
    "train_loader = utils.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "val_loader = utils.DataLoader(val_dataset, batch_size=val_batch_size, shuffle=True)\n",
    "test_loader = utils.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, labels):\n",
    "    binary_preds = (predictions >= 0.5).float()\n",
    "    correct = (binary_preds == labels).float()\n",
    "    accuracy = correct.mean() * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use(epochs):\n",
    "    train_accs, val_accs = [], []\n",
    "    best_acc, total_acc, total_val_acc = 0.0, 0.0, 0.0\n",
    "    \n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(images).squeeze()\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, labels)\n",
    "            total_acc += acc\n",
    "\n",
    "        train_acc = total_acc / (len(train_loader))\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                val_pred = model(images).squeeze()\n",
    "                val_acc = calculate_accuracy(val_pred, labels)\n",
    "                total_val_acc += val_acc\n",
    "\n",
    "        val_acc = total_val_acc / len(len(val_loader))\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "    test_predictions = []\n",
    "    with h5py.File(TEST_IMAGES_PATH, 'r') as f:\n",
    "        idx = list(f.keys())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images in test_loader:\n",
    "            images = images.to(device)\n",
    "            test_pred = (model(images).squeeze() > 0.5).float()\n",
    "            test_predictions.append(test_pred)\n",
    "        test = list(torch.cat(test_predictions).cpu().numpy())\n",
    "\n",
    "    return list(zip(idx, test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = use(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, columns=['ID', 'Pred'])\n",
    "df.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
